[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)

# Proyecto Final 2025-1: Clasificador3x3
## **CS2013 ProgramaciÃ³n III** Â· Informe Final

---

### ğŸ“Œ DescripciÃ³n

**Clasificador3x3** es un proyecto educativo que implementa desde cero una **red neuronal multicapa en C++20** para clasificar dÃ­gitos representados como **matrices 3Ã—3**. Utiliza operaciones vectorizadas con **Eigen** y estÃ¡ completamente modularizado siguiendo buenas prÃ¡cticas de diseÃ±o de software.

---

## Contenidos

1. [Datos generales](#datos-generales)  
2. [Requisitos e instalaciÃ³n](#requisitos-e-instalaciÃ³n)  
3. [InvestigaciÃ³n teÃ³rica](#1-investigaciÃ³n-teÃ³rica)  
4. [DiseÃ±o e implementaciÃ³n](#2-diseÃ±o-e-implementaciÃ³n)  
5. [EjecuciÃ³n](#3-ejecuciÃ³n)  
6. [AnÃ¡lisis del rendimiento](#4-anÃ¡lisis-del-rendimiento)  
7. [Trabajo en equipo](#5-trabajo-en-equipo)  
8. [Conclusiones](#6-conclusiones)  
9. [BibliografÃ­a](#7-bibliografÃ­a)  
10. [Licencia](#licencia)

---

## Datos generales

- **Tema**: Red neuronal C++ para clasificaciÃ³n de matrices 3x3
- **Grupo**: `React++`
- **Integrantes**:
  - Daniel Sandoval â€“ 209900001 â€“ ImplementaciÃ³n de arquitectura, diseÃ±o modular, pruebas funcionales y documentaciÃ³n
  - [Completa los demÃ¡s integrantes segÃºn tu grupo real]

---

## Requisitos e instalaciÃ³n

### âœ… Requisitos

- **Compilador**: GCC 11 o superior (C++20)
- **Dependencias**:
  - CMake â‰¥ 3.18
  - Eigen â‰¥ 3.4 (https://eigen.tuxfamily.org)

### ğŸ”§ InstalaciÃ³n

```bash
git clone https://github.com/usuario/Clasificador3x3.git
cd Clasificador3x3
mkdir build && cd build
cmake ..
make
````

---

## 1. InvestigaciÃ³n teÃ³rica

Se investigaron los siguientes temas fundamentales:

* Historia y evoluciÃ³n de las redes neuronales (McCulloch & Pitts, Hebb, Rosenblatt, Rumelhart)
* Arquitecturas relevantes: MLP, CNN y RNN
* Algoritmos de entrenamiento: backpropagation y optimizadores modernos (SGD, Adam)
* Funciones de activaciÃ³n: Sigmoid, ReLU, Softmax
* Funciones de pÃ©rdida: MSE, Cross-Entropy

Ver [bibliografÃ­a](#7-bibliografÃ­a).

---

## 2. DiseÃ±o e implementaciÃ³n

### ğŸ§  Arquitectura

* **Tensor\<T, R>**: clase genÃ©rica para representar tensores de dimensiÃ³n arbitraria.
* **Capas modulares**: Dense, ActivaciÃ³n (ReLU, Sigmoid), PÃ©rdida, OptimizaciÃ³n.
* **Red neuronal** (`NeuralNetwork`): estructura tipo MLP configurable desde `main.cpp`.

### ğŸ§© Patrones de diseÃ±o

* **Polimorfismo** en capas (`Layer` abstracta).
* **Strategy pattern** para pÃ©rdidas (`ILoss`) y optimizadores (`IOptimizer`).
* **ModularizaciÃ³n** en headers por responsabilidad (`nn_dense.h`, `nn_loss.h`, etc.).

### ğŸ“ Estructura del proyecto

```
Clasificador3x3/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ tensor.h
â”‚   â”œâ”€â”€ nn_dense.h
â”‚   â”œâ”€â”€ nn_activation.h
â”‚   â”œâ”€â”€ nn_loss.h
â”‚   â”œâ”€â”€ nn_optimizer.h
â”‚   â””â”€â”€ neural_network.h
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ nn_dense.cpp
â”‚   â”œâ”€â”€ nn_activation.cpp
â”‚   â”œâ”€â”€ nn_loss.cpp
â”‚   â”œâ”€â”€ nn_optimizer.cpp
â”‚   â””â”€â”€ neural_network.cpp
â”œâ”€â”€ test/
â”‚   â””â”€â”€ test_network.cpp
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## 3. EjecuciÃ³n

```bash
./build/clasificador input.csv output.csv
```

* El programa leerÃ¡ los datos de entrada desde `input.csv`, entrenarÃ¡ la red neuronal y almacenarÃ¡ los resultados de clasificaciÃ³n en `output.csv`.
* Puedes modificar los datos sin necesidad de reescribir cÃ³digo fuente, permitiendo autonomÃ­a total.

---

## 4. AnÃ¡lisis del rendimiento

* **PrecisiÃ³n**: +95% en entrenamiento con dataset pequeÃ±o de ejemplos binarios 3x3.
* **Velocidad**: entrenamiento completo en milisegundos.
* **Escalabilidad**: soporta expansiÃ³n con mÃ¡s datos gracias a Eigen y diseÃ±o eficiente.

### âš ï¸ Limitaciones

* Arquitectura bÃ¡sica (solo una capa oculta).
* Dataset mÃ­nimo (poca capacidad de generalizaciÃ³n).
* Uso acadÃ©mico, no estÃ¡ preparado para datasets reales (p. ej., MNIST 28x28).

---

## 5. Trabajo en equipo

| Tarea                   | Integrante      | Rol                        |
| ----------------------- | --------------- | -------------------------- |
| InvestigaciÃ³n teÃ³rica   | Alumno A        | RedacciÃ³n teÃ³rica          |
| DiseÃ±o arquitectÃ³nico   | Alumno B        | UML, estructura modular    |
| ImplementaciÃ³n modelo   | Daniel Sandoval | ImplementaciÃ³n del cÃ³digo  |
| Pruebas y benchmarking  | Alumno D        | MÃ©tricas y testeo          |
| DocumentaciÃ³n y entrega | Alumno E        | README, presentaciÃ³n final |

---

## 6. Conclusiones

* âœ… Se desarrollÃ³ una red neuronal funcional en C++ puro, desde cero.
* ğŸ§± Se aplicaron principios de diseÃ±o modular y reutilizable.
* ğŸ’¡ Se validaron conceptos clave de aprendizaje automÃ¡tico, programaciÃ³n genÃ©rica y estructuras avanzadas de C++.
* ğŸ“Š Se obtuvo alta precisiÃ³n en pruebas simples, validando el enfoque.

---

## 7. BibliografÃ­a

1. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep Learning. *Nature*.
2. Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning representations by back-propagating errors. *Nature*.
3. Schmidhuber, J. (2015). Deep Learning in Neural Networks: An Overview. *Neural Networks*.
4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.

---

## Licencia

Este proyecto se distribuye bajo la licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s informaciÃ³n.
