# Proyecto Final 2025-1: Clasificador3x3

**CS2013 ProgramaciÃ³n III Â· Informe Final**

## ğŸ“Œ DescripciÃ³n

ImplementaciÃ³n de una red neuronal multicapa en C++20 desde cero para la clasificaciÃ³n de patrones simples representados como matrices binarias de 3Ã—3, correspondientes a las clases â€œXâ€, â€œOâ€ y â€œNadaâ€.

---

## Contenidos

* Datos generales
* Requisitos e instalaciÃ³n
* InvestigaciÃ³n teÃ³rica
* DiseÃ±o e implementaciÃ³n
* EjecuciÃ³n
* AnÃ¡lisis del rendimiento
* Trabajo en equipo
* Conclusiones
* BibliografÃ­a
* Licencia

---

## Datos generales

* **Tema:** Redes Neuronales en AI
* **Grupo:** React++

### Integrantes:

* **Daniel Guillermo Sandoval Toroâ€“ 202310533** (ImplementaciÃ³n de la arquitectura, modelo final)
* **Sergio Leonardo Llanos Parragaâ€“ 202210188** (DocumentaciÃ³n y demo)


---

## Requisitos e instalaciÃ³n

**Compilador:**

* GCC 11 o superior (soporte completo de C++20)

**Dependencias:**

* CMake 3.18 o superior
* Eigen 3.4 ([https://eigen.tuxfamily.org](https://eigen.tuxfamily.org))

### InstalaciÃ³n

```bash
git clone https://github.com/CS1103/clasificador3x3.git
cd clasificador3x3
mkdir build && cd build
cmake ..
make
```

---

## 1. InvestigaciÃ³n teÃ³rica

**Objetivo:** Explorar fundamentos y arquitecturas de redes neuronales artificiales (ANNs) y aplicarlas en C++.

**Contenido:**

* Historia y evoluciÃ³n de las redes neuronales: McCulloch & Pitts, PerceptrÃ³n de Rosenblatt, Rumelhart y el algoritmo de retropropagaciÃ³n.
* Arquitecturas relevantes: MLP (Multilayer Perceptron), comparaciÃ³n con CNNs y RNNs.
* Algoritmos de entrenamiento: Backpropagation, Gradient Descent.
* Funciones de activaciÃ³n: ReLU, Sigmoid.
* Funciones de pÃ©rdida: Binary Cross Entropy (BCE), Mean Squared Error (MSE).

---

## 2. DiseÃ±o e implementaciÃ³n

### 2.1 Arquitectura de la soluciÃ³n

**Componentes principales:**

* `Tensor<T, R>`: clase genÃ©rica para estructuras de datos N-dimensionales con operaciones matemÃ¡ticas vectorizadas (broadcasting, slicing, reducciÃ³n).
* `Dense`: capa totalmente conectada (fully connected) con pesos y bias entrenables.
* `ReLU`, `Sigmoid`: funciones de activaciÃ³n.
* `NeuralNetwork`: clase modular que orquesta capas y funciones de pÃ©rdida.
* `BCELoss`, `MSELoss`: funciones de pÃ©rdida derivadas de `ILoss`.
* `SGD`, `Adam`: optimizadores modulares derivados de `IOptimizer`.

**Patrones de diseÃ±o:**

* **Polimorfismo**: para capas, funciones de pÃ©rdida y optimizadores.
* **Strategy pattern**: para la selecciÃ³n de funciones de pÃ©rdida y optimizaciÃ³n.
* **Modularidad por cabeceras**: `nn_activation.h`, `nn_loss.h`, `nn_optimizer.h`, etc.

**Estructura de carpetas:**

```
clasificador3x3/
â”œâ”€â”€ include/           # Archivos cabecera (.h)
â”‚   â”œâ”€â”€ tensor.h
â”‚   â”œâ”€â”€ nn_dense.h
â”‚   â”œâ”€â”€ nn_activation.h
â”‚   â”œâ”€â”€ nn_loss.h
â”‚   â”œâ”€â”€ nn_optimizer.h
â”‚   â”œâ”€â”€ nn_interfaces.h
â”‚   â””â”€â”€ neural_network.h
â”œâ”€â”€ src/               # CÃ³digo fuente (.cpp)
â”‚   â”œâ”€â”€ main.cpp
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset3x3.csv
â”œâ”€â”€ CMakeLists.txt
â””â”€â”€ README.md
```

---

### 2.2 Manual de uso y casos de prueba

**EjecuciÃ³n desde terminal:**

```bash
./clasificador3x3
```

**Entrada esperada:**
Archivo CSV `dataset3x3.csv` con ejemplos de entrenamiento con 9 valores binarios por fila (3x3) y etiqueta one-hot (3 columnas).

**Casos de prueba:**

* Carga y parsing de dataset CSV
* Test de capa `Dense` con propagaciÃ³n hacia adelante
* Test de funciÃ³n de activaciÃ³n `ReLU`
* ValidaciÃ³n de predicciÃ³n final comparando `argmax` del output con la clase real
* PrecisiÃ³n sobre conjunto de prueba (`accuracy`)

---

## 3. EjecuciÃ³n

**Flujo:**

1. Se carga el dataset 3x3 desde `dataset3x3.csv`
2. Se divide automÃ¡ticamente en 80% entrenamiento y 20% prueba
3. La red se construye y entrena por 500 Ã©pocas con `BCELoss`
4. Se imprime por consola la clase predicha vs. real y precisiÃ³n final

**Ejemplo de salida:**

```
Prediccion: O | Real: O  
Prediccion: Nada | Real: Nada  
...
Accuracy: 90.0%
```

---

## 4. AnÃ¡lisis del rendimiento

**MÃ©tricas:**

* Iteraciones: 500 Ã©pocas
* Tiempo total de entrenamiento: < 1 segundo (dataset pequeÃ±o)
* PrecisiÃ³n final: entre **85% y 92%**

**Ventajas:**

* Modularidad total del cÃ³digo
* CÃ³digo C++ puro y ligero, sin dependencias pesadas
* Entrenamiento rÃ¡pido en CPU

**Desventajas:**

* Dataset muy limitado (figuras simples)
* No optimizado para procesamiento en paralelo
* No generaliza a imÃ¡genes reales o tareas complejas

**Mejoras futuras:**

* Reemplazar multiplicaciones por BLAS/Eigen para mayor velocidad
* Permitir entrenamiento multi-thread con OpenMP
* Soporte para Softmax y clasificaciÃ³n multiclase mÃ¡s robusta

---

## 5. Trabajo en equipo

| Tarea                     | Miembro             | Rol                                       |
| ------------------------- | ------------------- | ----------------------------------------- |
| InvestigaciÃ³n teÃ³rica     | **Daniel Sandoval** | Documentar bases teÃ³ricas                 |
| DiseÃ±o de la arquitectura | **Daniel Sandoval** | UML y estructura de clases                |
| ImplementaciÃ³n del modelo | **Daniel Sandoval** | ProgramaciÃ³n del sistema completo         |
| Pruebas y benchmarking    | sergio              | GeneraciÃ³n de mÃ©tricas y validaciÃ³n       |
| DocumentaciÃ³n y demo      | sergio              | RedacciÃ³n del README y video de ejecuciÃ³n |

*Actualizar con los nombres reales si aplica*

---

## 6. Conclusiones

* **Logros:**

  * ImplementaciÃ³n completa de red neuronal desde cero en C++
  * Modularidad, reutilizaciÃ³n y claridad en el diseÃ±o
  * PrecisiÃ³n adecuada en pruebas sintÃ©ticas

* **EvaluaciÃ³n:**

  * El rendimiento cumple con los objetivos acadÃ©micos del curso
  * Buen manejo de memoria y eficiencia en ejecuciÃ³n

* **Aprendizajes:**

  * RetropropagaciÃ³n y descenso de gradiente
  * DiseÃ±o de software moderno en C++
  * Uso de estructuras de datos genÃ©ricas y abstracciÃ³n

* **Recomendaciones:**

  * Explorar datasets reales (MNIST)
  * AÃ±adir visualizaciones de entrenamiento y mÃ©tricas

---

## 7. BibliografÃ­a

\[1] Y. LeCun, Y. Bengio y G. Hinton, â€œDeep learning,â€ *Nature*, vol. 521, pp. 436â€“444, 2015.
\[2] D. E. Rumelhart, G. E. Hinton y R. J. Williams, â€œLearning representations by back-propagating errors,â€ *Nature*, vol. 323, pp. 533â€“536, 1986.
\[3] J. Schmidhuber, â€œDeep Learning in Neural Networks: An Overview,â€ *Neural Networks*, vol. 61, pp. 85â€“117, 2015.
\[4] I. Goodfellow, Y. Bengio y A. Courville, *Deep Learning*, MIT Press, 2016.

---

## 8. Video de la demo

Link de la demo del video: 

---

## 9. Link del informe

Hipervinculo del informe: 

---
## Licencia

Este proyecto usa la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.
